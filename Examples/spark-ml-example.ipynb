{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b28b00a-d73f-4c6a-8345-037a6dab8711",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook provides a top-level technical introduction to combining Apache Spark with MongoDB, enabling developers and data engineers to bring sophisticated real-time analytics and machine learning to live, operational data.\n",
    "\n",
    "The following illustrates how to use MongoDB and Spark with an example application that uses Spark's alternating least squares (ALS) implementation to generate a list of movie recommendations for a user.\n",
    "\n",
    "The following tasks will be performed:\n",
    "* Read data from MongoDB into Spark.\n",
    "* Run the MongoDB Connector for Spark in Spark.\n",
    "* Use the machine learning ALS library in Spark to generate a set of personalized movie recommendations for a given user.\n",
    "* Write the recommendations back to MongoDB so they are accessible to applications.\n",
    "\n",
    "## Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48d81485-bf32-46ab-a8fa-132da3678f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac56cca-5582-4f7e-9621-b1ba6ef54765",
   "metadata": {},
   "source": [
    "# Read data from MongoDB\n",
    "The Spark Connector can be configured to read from MongoDB in a number of ways.\n",
    "We use in this notebook the SparkSesssion object directly, via an options map. The SparkSession reads from the \"ratings\" collection in the \"recommendation\" database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35779078-ef49-46a7-8652-ccbe117846cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Spark session\n",
    "spark = SparkSession.\\\n",
    "builder.\\\n",
    "appName(\"pyspark-notebook2\").\\\n",
    "    master(\"spark://spark-master:7077\").\\\n",
    "    config(\"spark.executor.memory\", \"1g\").\\\n",
    "    config(\"spark.mongodb.input.uri\",\"mongodb://mongo1:27017,mongo2:27018,mongo3:27019/recommendation.ratings?replicaSet=rs0\").\\\n",
    "    config(\"spark.mongodb.output.uri\",\"mongodb://mongo1:27017,mongo2:27018,mongo3:27019/recommendation.ratings?replicaSet=rs0\").\\\n",
    "    config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\").\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7212de5f-34ca-44dc-a769-e31713164e61",
   "metadata": {},
   "source": [
    "One of the most attractive features of MongoDB is support for flexible schemas, which enables you to store a variety of JSON models in the same collection. A consequence of flexible schemas is that there is no defined schema for a given collection as there would be in an RDBMS. Since DataFrames and Datasets require a schema, the Spark Connector will automatically infer the schema by randomly sampling documents from the database. It then assigns that inferred schema to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691cee35-a13c-4143-8fcf-90d3513f8219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data into Spark\n",
    "ratings_df = spark.read.format(\"mongo\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b86b7-e9fd-41b9-b377-dfcf149db236",
   "metadata": {},
   "source": [
    "The Spark Connector's ability to infer schema through document sampling is a nice convenience, but if you know your document structure you can assign the schema explicitly and avoid the need for sampling queries. The following example shows you how to define a DataFrame's schema explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c1ba75b-c3ed-413d-839f-31e1c1c39873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Looking at the schema dataset\n",
    "ratings_df.cache() \n",
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f068a41e-72ce-43c1-b83f-aa83b62e56ad",
   "metadata": {},
   "source": [
    "Lets now look at the data which compose the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e50b91bf-baf9-417f-a440-d4aaaf822b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movieId</th>\n",
       "      <td>100836</td>\n",
       "      <td>19435.2957177992</td>\n",
       "      <td>35530.9871987004</td>\n",
       "      <td>1</td>\n",
       "      <td>193609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <td>100836</td>\n",
       "      <td>3.501556983616962</td>\n",
       "      <td>1.0425292390606307</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <td>100836</td>\n",
       "      <td>1.2059460873684695E9</td>\n",
       "      <td>2.1626103599513054E8</td>\n",
       "      <td>828124615</td>\n",
       "      <td>1537799250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <td>100836</td>\n",
       "      <td>326.12756356856676</td>\n",
       "      <td>182.6184914634992</td>\n",
       "      <td>1</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0                     1                     2          3  \\\n",
       "summary     count                  mean                stddev        min   \n",
       "movieId    100836      19435.2957177992      35530.9871987004          1   \n",
       "rating     100836     3.501556983616962    1.0425292390606307        0.5   \n",
       "timestamp  100836  1.2059460873684695E9  2.1626103599513054E8  828124615   \n",
       "userId     100836    326.12756356856676     182.6184914634992          1   \n",
       "\n",
       "                    4  \n",
       "summary           max  \n",
       "movieId        193609  \n",
       "rating            5.0  \n",
       "timestamp  1537799250  \n",
       "userId            610  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a descriptive analysis\n",
    "ratings_df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4eba14-bb55-4430-92ee-20ae71b5a169",
   "metadata": {},
   "source": [
    "# Using Machine Learning librairies\n",
    "we use the ALS library for Apache Spark to learn our dataset in order to make predictions for a user. This example is detailed as a five step process. You can learn more about how ALS generates predictions in the Spark documentation which is not the purpose of this notebook.\n",
    "\n",
    "## Creation of the Machine Learning Model\n",
    "For training purposes, the complete data set must also split into smaller partitions known as the training, validation, and test data. In this case, 80% of the data will be used for training and the rest can be used to validate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a88d0124-080c-4d8f-ab29-79dbf811bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into 80/20 for training and test\n",
    "splits = ratings_df.randomSplit([0.8,0.2]) \n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a357a0-6dd7-4152-a30f-29cb09aa791c",
   "metadata": {},
   "source": [
    "Recommendation model are built on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34168e17-8af2-4709-ab0f-0b0067dd9a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS().setMaxIter(5).setRegParam(0.01).setUserCol(\"userId\").\\\n",
    "    setItemCol(\"movieId\").setRatingCol(\"rating\")\n",
    "model = als.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35f8c1c-032f-4333-98b8-95ff10ef3569",
   "metadata": {},
   "source": [
    "To check the accuracy of the training model, we compute the RMSE (Root Mean Square Error) on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "694df087-634c-4ef5-8625-4d5e0e881cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error =  1.1018619243577332\n"
     ]
    }
   ],
   "source": [
    "# Setting cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "model.setColdStartStrategy(\"drop\") \n",
    "predictions = model.transform(test_df)\n",
    "evaluator = RegressionEvaluator().setMetricName(\"rmse\").setLabelCol(\"rating\").\\\n",
    "    setPredictionCol(\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions) \n",
    "print(\"Root-mean-square error = \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7484e830-06fe-4c7b-8315-f87ec286d838",
   "metadata": {},
   "source": [
    "Lets display the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e2ef7a8-5213-4cd8-ac05-5535edac4247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------+----------+------+----------+\n",
      "|                 _id|movieId|rating| timestamp|userId|prediction|\n",
      "+--------------------+-------+------+----------+------+----------+\n",
      "|{60c0d3fc217b5825...|    471|   4.0| 843491793|   133| 2.2460139|\n",
      "|{60c0d3fd217b5825...|    471|   2.0| 941558175|   597|  4.226793|\n",
      "|{60c0d3fd217b5825...|    471|   3.0| 833530187|   436| 3.2405984|\n",
      "|{60c0d3fd217b5825...|    471|   3.0| 874415126|   372| 3.2151067|\n",
      "|{60c0d3fc217b5825...|    471|   4.0|1111624874|   218| 2.0807343|\n",
      "|{60c0d3fd217b5825...|    471|   4.0|1479544381|   610| 3.2153044|\n",
      "|{60c0d3fd217b5825...|    471|   4.0|1178980875|   448|  4.650613|\n",
      "|{60c0d3fc217b5825...|    471|   4.0|1043175564|   312| 2.2405844|\n",
      "|{60c0d3fd217b5825...|    471|   5.0| 965425364|   469| 2.0484393|\n",
      "|{60c0d3fd217b5825...|    471|   5.0| 961514069|   414| 2.7908955|\n",
      "|{60c0d3fd217b5825...|    471|   1.5|1117161794|   608| 1.1752882|\n",
      "|{60c0d3fc217b5825...|    471|   4.5|1109409455|   260|  3.642147|\n",
      "|{60c0d3fc217b5825...|   1088|   4.0|1508641161|   159|  2.905725|\n",
      "|{60c0d3fd217b5825...|   1088|   2.5|1498515232|   599| 2.6863503|\n",
      "|{60c0d3fc217b5825...|   1088|   4.0|1329984080|   132| 2.3107216|\n",
      "|{60c0d3fd217b5825...|   1088|   3.5|1100292226|   474| 3.1800044|\n",
      "|{60c0d3fb217b5825...|   1088|   4.0|1161559902|    64|  3.620729|\n",
      "|{60c0d3fd217b5825...|   1088|   4.0|1440793700|   563| 2.4862173|\n",
      "|{60c0d3fc217b5825...|   1088|   3.0|1186162146|   307|0.93635845|\n",
      "|{60c0d3fc217b5825...|   1088|   4.5|1337195649|   116| 4.2883377|\n",
      "+--------------------+-------+------+----------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display predictions\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8fefd3-a6aa-45bc-9b32-e6ca17cd499b",
   "metadata": {},
   "source": [
    "Finally, we use the model to generate a set of 10 recommended movies for each user in the dataset, and write those recommendations back to MongoDB.\n",
    "To do so we will firt clean-up the generated dataframe prior to to write it back into MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c07046e2-4d43-4d45-85e2-ad700b4d6312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the model to generate a set of 10 recommended movies for each user in the dataset\n",
    "columns = [\"_id\", \"timestamp\"] \n",
    "docs = predictions.drop(*columns)\n",
    "\n",
    "# Need to cast predictions from FloatType to Double as Float is not a BSON type in MongoDB\n",
    "docs = docs.withColumn(\"prediction\", docs.prediction.cast(DoubleType())) \n",
    "docs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfbb647-e9bf-496b-bb85-1b6d67acfd27",
   "metadata": {},
   "source": [
    "Lets write now the recommendations to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2e54ea5-535b-4a03-80bc-597b3b3cf63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write recommendations to MongoDB\n",
    "docs.write.format(\"mongo\").mode(\"overwrite\").option(\"database\", \"recommendation\").option(\"collection\", \"recommendations\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cb47f4-3f81-4b9f-9360-9f701b16204d",
   "metadata": {},
   "source": [
    "Lets check the result by reading the recommendation for a specific user from MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c614fd3-5251-4181-ae9a-865660ea3cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|movieId|rating|userId|\n",
      "+-------+------+------+\n",
      "|    333|   5.0|     1|\n",
      "|    231|   5.0|     1|\n",
      "|    151|   5.0|     1|\n",
      "|     50|   5.0|     1|\n",
      "|    101|   5.0|     1|\n",
      "|    362|   5.0|     1|\n",
      "|    260|   5.0|     1|\n",
      "|    457|   5.0|     1|\n",
      "|    157|   5.0|     1|\n",
      "|     47|   5.0|     1|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read recommendations for a user\n",
    "pipeline = \"[{'$match': {'userId': 1}}, {'$project': {'_id': 0, 'timestamp': 0}}, {'$sort': {'rating': -1}}, {'$limit': 10}]\"\n",
    "aggPipelineDF = spark.read.format(\"mongo\").option(\"pipeline\", pipeline).option(\"partitioner\", \"MongoSinglePartitioner\").load()\n",
    "aggPipelineDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f412ff-bc3f-4f8e-a5d8-248af1cfa78c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
